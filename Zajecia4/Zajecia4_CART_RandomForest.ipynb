{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4 - Tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading and pre-processing dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use IMDB 5000 Movies dataset in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"IMDB.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive data report may be generated using `pandas_profiling` package - **IMDB_Report.zip** contains profiling report for IMDB dataset. More information about the package can be found on [Pandas-profiling official docs](https://pandas-profiling.ydata.ai/docs/master/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(dataset, title = \"IMDB Movies Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(output_file = \"IMDB_Report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Inspecting columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking numeric columns\n",
    "dataset.describe(include = [np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking nominal columns\n",
    "dataset.describe(include = ['O']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with many unique values and imbalanced classes\n",
    "dataset.drop(['color', 'director_name', 'actor_2_name', 'actor_1_name',\n",
    "             'movie_title', 'actor_3_name', 'plot_keywords',\n",
    "             'movie_imdb_link', 'language', 'country', 'content_rating'],\n",
    "             axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "print(dataset.shape)\n",
    "dataset.drop_duplicates(inplace = True)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping missing values\n",
    "dataset.dropna(inplace = True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial EDA (Exploratory Data Analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dataset = dataset.drop(\"genres\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize = [15, 15])\n",
    "plt.tight_layout(pad = 0.4, w_pad = 1.0, h_pad = 1.0)\n",
    "for n,col in enumerate(numeric_dataset):\n",
    "    sns.regplot(x = col, y = \"imdb_score\", data = dataset, ax = axes[n//4, n%4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize = [15,15])\n",
    "plt.tight_layout(pad = 0.4, w_pad = 1.0, h_pad = 1.0)\n",
    "for n,col in enumerate(numeric_dataset):\n",
    "    sns.boxplot(x = col, data = dataset, ax = axes[n//4, n%4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting outliers**\n",
    "\n",
    "A raw score x is converted into a standard score by\n",
    "\n",
    "$$ z= \\frac{x-\\mu}{\\sigma}  $$\n",
    "\n",
    "where:\n",
    "\n",
    "* μ is the mean of the population,\n",
    "* σ is the standard deviation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.zscore(numeric_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers\n",
    "dataset = dataset[(np.abs(stats.zscore(numeric_dataset)) < 9).all(axis = 1)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting genres column values\n",
    "dataset['genres'] = dataset.genres.str.split(\"|\")\n",
    "dataset['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting distinct categories\n",
    "categories = set(dataset.genres.explode())\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encode each movie's classification\n",
    "for cat in categories:\n",
    "    dataset[cat] = dataset.genres.apply(lambda s: int(cat in s))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop genres column\n",
    "dataset.drop('genres', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA on cleaned data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 4, figsize = [15, 15])\n",
    "plt.tight_layout(pad = 0.4, w_pad = 1.0, h_pad = 1.0)\n",
    "for n,col in enumerate(dataset.columns[0:16]):\n",
    "    sns.regplot(x = col, y = \"imdb_score\", data = dataset, ax = axes[n//4, n%4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(6, 4, figsize = [15, 15])\n",
    "plt.tight_layout(pad = 0.4, w_pad=1.0, h_pad = 1.0)\n",
    "for n, col in enumerate(dataset.columns[16:]):\n",
    "    sns.barplot(x = col, y = \"imdb_score\", data = dataset, ax = axes[n//4, n%4]).set_ylim(5,)\n",
    "f.delaxes(axes[5, 2]); f.delaxes(axes[5, 3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('imdb_score', axis = 1)\n",
    "y = dataset['imdb_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: [sklearn docs](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart)\n",
    "\n",
    "_What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?_\n",
    "\n",
    "**ID3** (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "**C4.5** is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "**C5.0** is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
    "\n",
    "**CART** (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "\n",
    "**scikit-learn uses an optimised version of the CART algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART algorithm pick variables and cutoff threshold using:\n",
    " 1. __for classification__: minimization of node's heterogeneity (Gini index or entropy) \n",
    " 2. __for regression__: minimizing error of prediction (e.g. sum of squares of residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CART = tree.DecisionTreeRegressor(random_state = 42, ccp_alpha = 0.0)\n",
    "CART_model = CART.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CART_model.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruning CART tree (cost based)**\n",
    "\n",
    "[Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = CART.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas[::5], path.impurities[::5]\n",
    "figure(figsize = (12, 8))\n",
    "plt.plot(ccp_alphas[:-1], impurities[:-1], marker = 'o', drawstyle = \"steps-post\")\n",
    "plt.xlabel(\"Cost parameter\", fontsize = 15)\n",
    "plt.ylabel(\"Total impurity of leaves\", fontsize = 15)\n",
    "plt.title(\"Total Impurity vs complexity hyperparameter for training set\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeRegressor(random_state = 42, ccp_alpha = ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order, ind in [('First', 0), ('Last', -1)]:\n",
    "    print(f\"{order} tree with ccp_alpha: {ccp_alphas[ind]:.2f}, \" +\n",
    "          f\"nodes: {clfs[ind].tree_.node_count}, leaves: {clfs[ind].get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(model, X, y):\n",
    "    return np.sqrt(((model.predict(X) - y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = [RMSE(clf, X_test, y_test) for clf in clfs]\n",
    "train_scores = [RMSE(clf, X_train, y_train) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = [12, 8])\n",
    "ax.set_xlabel(\"Alpha\", fontsize = 15)\n",
    "ax.set_ylabel(\"RMSE\", fontsize = 15)\n",
    "ax.set_title(\"RMSE vs alpha for training and test sets\", fontsize = 20)\n",
    "ax.plot(ccp_alphas, train_scores, marker = 'o', label = \"train\", drawstyle = \"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker = 'o', label = \"test\", drawstyle = \"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complexity (cost) that produce the best regression tree\n",
    "Best_CART = clfs[np.argmin(test_scores)]\n",
    "Best_CART.ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the tree\n",
    "plt.figure(figsize = (15, 10))\n",
    "_ = tree.plot_tree(Best_CART, #clfs[-1] \n",
    "                   feature_names = X_train.columns,  \n",
    "                   filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE of the best tree\n",
    "min(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#But we can view this as multiclass classification\n",
    "confmat = pd.crosstab(Best_CART.predict(X_test).round(), y_test.round())\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "np.array([confmat.loc[i,i] for i in confmat.index]).sum()/confmat.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble tree-based methods\n",
    "\n",
    "[Ensemble learning](https://scikit-learn.org/stable/modules/ensemble.html) helps improve final model performance by combining results of underlying models (e.g. random forest is combination of decision trees).\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "- In **averaging methods**, the main principle is to build several estimators **independently** and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "> Example: Random forests\n",
    "\n",
    "- By contrast, in **boosting methods**, base estimators are built **sequentially** and the following models tries to reduce the error of the combined estimator.\n",
    "\n",
    "> Example: Boosted trees\n",
    "\n",
    "<img src=\"https://hpccsystems.com/wp-content/uploads/2022/09/LearningTrees-1.png\" width=500>\n",
    "\n",
    "[Source](https://hpccsystems.com/resources/learning-trees-a-guide-to-decision-tree-based-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forests**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#random-forests\n",
    "\n",
    "Random forest is a collection of 'weak' decision trees providing good performance together.\n",
    "\n",
    "Trees are weakned using multiple techniques:\n",
    "* bootstrap sample, potentially on subset of available data\n",
    "* limiting number of features\n",
    "* no pruning\n",
    "\n",
    "We'll look on two parameters in `RandomForestRegressor` class:\n",
    "- n_estimators - number of trees built in ensemble\n",
    "- max_features - how many features will be included in each tree e.g.\n",
    "    - 'auto' - all\n",
    "    - 'sqrt' - random sample of sqrt(n_features)\n",
    "    - n - random sample of 'n' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of tress influence on RMSE\n",
    "rfr = RandomForestRegressor\n",
    "N = [10, 50, 100, 200, 300, 400, 500]\n",
    "RMSE_RF= [RMSE(rfr(n, n_jobs = -1).fit(X_train, y_train), X_test, y_test) for n in N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize = (12, 8))\n",
    "plt.plot(N, RMSE_RF, '.-', color = 'g');\n",
    "plt.xlabel(\"# trees\", fontsize = 15)\n",
    "plt.ylabel(\"RMSE\", fontsize = 15)\n",
    "print(\"Minimum for\", N[np.argmin(RMSE_RF)], \"trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of features influence on RMSE\n",
    "features = np.linspace(1, X_train.shape[1], 10).astype(int)\n",
    "RMSE_RF_features= [RMSE(rfr(400, max_features = n, n_jobs = -1).fit(X_train, y_train), X_test, y_test) for n in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize = (12, 8))\n",
    "plt.plot(features, RMSE_RF_features, '.-', color = 'r');\n",
    "features[np.argmin(RMSE_RF_features)]\n",
    "plt.xlabel(\"# features\", fontsize = 18)\n",
    "plt.ylabel(\"RMSE\", fontsize = 18)\n",
    "print(\"Minimum for\", features[np.argmin(RMSE_RF_features)], \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to handle the uncertainty in picking optimal values of hyperparameters?**\n",
    "\n",
    "[Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_RF = RandomForestRegressor(400, max_features = 25, n_jobs = -1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "importances = Best_RF.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in Best_RF.estimators_], axis = 0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "num_feat = 6\n",
    "plt.figure(figsize = [15, 8])\n",
    "plt.title(\"Feature importances\", fontsize = 20)\n",
    "plt.bar(range(num_feat)[:num_feat], importances[indices][:num_feat],\n",
    "       color = \"r\", yerr = std[indices][:num_feat], align = \"center\")\n",
    "plt.xticks(range(num_feat)[:num_feat], X_train.columns[indices[:num_feat]])\n",
    "plt.xlim([-1, num_feat])\n",
    "plt.ylabel(\"Impurity reduction\", fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Gradient Boosted Trees**](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking number of tress influence on RMSE\n",
    "gbr = GradientBoostingRegressor\n",
    "N = [10, 50, 100, 200, 300, 400, 500, 600, 700]\n",
    "RMSE_GBT = [RMSE(gbr(n_estimators = n).fit(X_train, y_train), X_test, y_test) for n in N]\n",
    "\n",
    "figure(figsize = (12, 8))\n",
    "plt.plot(N, RMSE_GBT, '.-', color = 'r');\n",
    "plt.xlabel(\"# trees\", fontsize = 18)\n",
    "plt.ylabel(\"RMSE\", fontsize = 18)\n",
    "print(\"Minimum for\", N[np.argmin(RMSE_GBT)], \"trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From 500 trees RMSE reduction is insignificant\n",
    "Best_GBT = GradientBoostingRegressor(n_estimators = 500).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feature_importance = Best_GBT.feature_importances_\n",
    "# Make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "num_feat = 6\n",
    "\n",
    "plt.figure(figsize = [12, 8])\n",
    "plt.barh(pos[-num_feat:], feature_importance[sorted_idx][-num_feat:], align='center', alpha = 0.75)\n",
    "plt.yticks(pos[-num_feat:], X_train.columns[sorted_idx][-num_feat:])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing results of Decision Tree, Random Forest and Gradient Boosted Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Best_CART, Best_RF, Best_GBT]\n",
    "errors = [RMSE(m, X_test, y_test) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(['CART','Random Forest','Gradient Boosted Trees'], errors, color = ['red', 'green', 'blue'], alpha = 0.75)\n",
    "plt.ylabel('RMSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
